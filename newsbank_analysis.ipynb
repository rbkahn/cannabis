{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vec_utils import display_highlights, get_model, closest_words, process_paragraphs, get_sentence_list\n",
    "from gensim.similarities.index import AnnoyIndexer\n",
    "from histwords import smart_procrustes_align_gensim as align\n",
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import statsmodels.api as sm\n",
    "from collections import Counter\n",
    "import random\n",
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_association(word, weed_word1=\"marijuana\", weed_word2=\"cannabis\"):\n",
    "    d = get_association(word, weed_word1)\n",
    "    lists = sorted(d.items()) # sorted by key, return a list of tuples\n",
    "    x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "    f, ax = plt.subplots(1)\n",
    "    ax.plot(x, y, label=weed_word1.lower())\n",
    "        #d2 = get_association(word, weed_word2)\n",
    "    #lists2 = sorted(d2.items()) # sorted by key, return a list of tuples\n",
    "    #x2, y2 = zip(*lists2) # unpack a list of pairs into two tuples\n",
    "    #ax.plot(x2, y2, label=weed_word2.lower())\n",
    "    plt.ylim(ymin=0, ymax=1)\n",
    "    plt.show()\n",
    "def get_association(word1, word2=\"marijuana\"):\n",
    "    aligned = {year : align(get_model(range(1980, 1985)), get_model(range(year, year+5))) for year in range(1980, 2020, 5)}\n",
    "    return {year : aligned[year].distance(word2.lower(), word1.lower()) for year in range(1980, 2020, 5)}\n",
    "def lin_reg(word, word2=\"marijuana\"):\n",
    "    d = get_association(word, word2)\n",
    "    X = list(d.keys())\n",
    "    Y = list(map(lambda x : 100 * x, d.values()))\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(Y,X).fit()\n",
    "    predictions = model.predict(X)\n",
    "    return model.params[1], model.bse[1]\n",
    "def get_intersect_vocab():\n",
    "    n = 1980\n",
    "    x = set(get_model(range(1980, 1985)).vocab)\n",
    "    for n in range(1985, 2020, 5):\n",
    "        x = x.intersection(set(get_model(range(n, n+5)).vocab))\n",
    "    return x\n",
    "\n",
    "def get_lr_df(intersect_vocab):\n",
    "    aligned = {year : align(get_model(range(1980, 1985)), get_model(range(year, year+5))) for year in range(1980, 2020, 5)}\n",
    "    def get_association(word1, word2=\"marijuana\"):\n",
    "        return {year : aligned[year].distance(word2.lower(), word1.lower()) for year in range(1980, 2020, 5)}\n",
    "    lr_dict = dict()\n",
    "    for word in intersect_vocab:\n",
    "        lr_dict[word] = dict()\n",
    "        coef, stder = lin_reg(word)\n",
    "        lr_dict[word]['coef'] = coef\n",
    "        lr_dict[word]['stder'] = stder\n",
    "    df = pd.DataFrame.from_dict(lr_dict).transpose()\n",
    "    df['lower'] = df['coef'] - df['stder']\n",
    "    df['upper'] = df['coef'] + df['stder']\n",
    "    counter = Counter()\n",
    "    for paragraph in process_paragraphs(get_df(range(1980, 2020))):\n",
    "        for word in paragraph:\n",
    "            counter[word] += 1\n",
    "    from random import random\n",
    "    counts = []\n",
    "    for word in df.index:\n",
    "        counts.append(counter[word])\n",
    "    df['count'] = counts\n",
    "    return df\n",
    "words = [\"heroin\", \"opium\", \"lsd\", \"cocaine\", \"crime\", \"criminal\", \"smuggling\", \"trafficking\", \"medical\", \"medicine\", \"recreational\", \"politics\", \"legalize\"] \n",
    "def plot_trends(words, df):\n",
    "    X = words\n",
    "    Y = [df.loc[word, 'coef'] for word in words]\n",
    "    std = [df.loc[word, 'stder'] for word in words]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.errorbar(X, Y, yerr=std, linestyle=\"\")\n",
    "    #ax.set_aspect('equal')\n",
    "    ax.axhline(y=0, color='k')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "get_sentence_list(years=range(1980, 1990), word_target=\"seniors\", only_weed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "He has just introduced a bill repealing all state regulation of marijuana, basically striking any mention of weed from the books. If passed, which is unlikely, the bill would radically propel the Lone Star State to the forefront of American marijuana advocacy and reform, leaving more cautious states, like Colorado and Washington, in the dust.\n0.8571428571428571\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "hs = 0\n",
    "ls = get_sentence_list(years=range(2010, 2020), word_target=\"dust\", only_weed=True)\n",
    "for p in ls:\n",
    "    if \"angel dust\" in p:\n",
    "        hs += 1\n",
    "    else:\n",
    "        print(p)\n",
    "print(hs/len(ls))\n",
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sig_shifts:  92.31\navg_coef:  0.653\n"
    }
   ],
   "source": [
    "# for my words, get the % of them that have significant meaning shifts, and get the absolute avg\n",
    "lr_df = pd.read_csv('lin_reg.csv', header=0, names=['word', 'coef', 'stder', 'sig', 'counnt'])\n",
    "sig_shifts = len([entry for entry in lr_df.loc[lr_df['word'].isin(words)]['sig'] if entry]) / len(words)\n",
    "avg_coef = mean([abs(coef) for coef in lr_df.loc[lr_df['word'].isin(words)]['coef']])\n",
    "print(\"sig_shifts: \", round(sig_shifts*100, 2))\n",
    "print(\"avg_coef: \", round(avg_coef, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sig_shifts:  64.91\navg_coef:  0.299\n"
    }
   ],
   "source": [
    "# for all words, get the % of them that have significant meaning shifts, and get the absolute avg\n",
    "sig_shifts = len([entry for entry in lr_df['sig'] if entry]) / len(lr_df)\n",
    "avg_coef = mean([abs(coef) for coef in lr_df['coef']])\n",
    "print(\"sig_shifts: \", round(sig_shifts*100, 2))\n",
    "print(\"avg_coef: \", round(avg_coef, 3))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'wordsamples' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-d6e8cb714644>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mword_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentry\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlr_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sig'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcoef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcoef\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlr_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'coef'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwordsamples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#for _ in range(5000):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m  \u001b[1;31m#   random_words = random.sample(list(lr_df['word']), len(words))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wordsamples' is not defined"
     ]
    }
   ],
   "source": [
    "# Do 5000 trials of 13 random words each and get expected % and sd to have sig meaning shifts and mean and std of absolute avg coef\n",
    "sig = []\n",
    "word_samples = [random.sample(list(lr_df['word']), len(words)) for _ in range(5000)]\n",
    "sig = [len([entry for entry in lr_df.loc[lr_df['word'].isin(words)]['sig'] if entry]) / len(words) for words in word_samples]\n",
    "coef = [mean([abs(coef) for coef in lr_df.loc[lr_df['word'].isin(words)]['coef']]) for words in wordsamples]\n",
    "#for _ in range(5000):\n",
    " #   random_words = random.sample(list(lr_df['word']), len(words))\n",
    " #   sig_shifts = len([entry for entry in lr_df.loc[lr_df['word'].isin(words)]['sig'] if entry]) / len(words)\n",
    " #   avg_coef = sum([abs(coef) for coef in lr_df.loc[lr_df['word'].isin(words)]['coef']])/len(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sig_shifts      mean: 0.646477   std: 0.135013\ncoef            mean: 0.299677   std: 0.063850\n"
    }
   ],
   "source": [
    "print(\"sig_shifts      mean: %f   std: %f\" % (mean(sig), stdev(sig)))\n",
    "print(\"coef            mean: %f   std: %f\" % (mean(coef), stdev(coef)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}