{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import urllib\n",
    "import requests\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from yanytapi import SearchAPI\n",
    "from gensim.similarities.index import AnnoyIndexer\n",
    "from gensim.models import Word2Vec\n",
    "from mittens import GloVe, Mittens\n",
    "from gensim.matutils import corpus2csc\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "#api = SearchAPI(\"TjGk9kxFO9ScvfSF8AfeqkXjjujBnz6e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_strings(s):\n",
    "    s = s.lower().replace(\".\", \"\").replace(\"'s\",\"\").replace(\"?\",\"\").replace(\"!\",\"\").replace(\",\", \"\").replace(\";\", \"\").replace(\"\\\"\", \"\").replace(\"”\", \"\").replace(\"“\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    if len(s) > 0 and s[0] == '$':\n",
    "        return '$'\n",
    "    return s\n",
    "    \n",
    "def csv_name(year):\n",
    "    return 'articles-' + str(year) + '.csv'\n",
    "\n",
    "def remove_waste(sentence):\n",
    "    wasted_words = ['—', '&']\n",
    "    return [word for word in sentence if word not in wasted_words]\n",
    "\n",
    "def co_occurrence(df, window=5):\n",
    "    print(\"co-occurrence\")\n",
    "    sentences = [remove_waste(list(map(lambda s : process_strings(s), p.split()))) for p in df['text']]\n",
    "    d = dict()\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] not in d:\n",
    "                d[sentence[i]] = defaultdict(int)\n",
    "            for j in range(-window, window):\n",
    "                if i+j >= 0 and i+j < len(sentence) and i != j: \n",
    "                    d[sentence[i]][sentence[i+j]] += 1\n",
    "    return d\n",
    "\n",
    "def trim_d(d):\n",
    "    print(\"trimming\")\n",
    "    vocab = list(d.keys())\n",
    "    print(len(vocab))\n",
    "    for word in d:\n",
    "        if sum([v for k, v in dict(d[word]).items()]) < 100:\n",
    "            vocab.remove(word)\n",
    "    print(len(vocab))\n",
    "    return {k:d[k] for k in vocab}\n",
    "\n",
    "def d_to_matrix(d):\n",
    "    print(\"matrixing\")\n",
    "    vocab = list(d.keys())\n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    for i in range(len(vocab)):\n",
    "        for j in range(len(vocab)):\n",
    "            matrix[i][j] = d[vocab[i]][vocab[j]]\n",
    "    return vocab, matrix\n",
    "\n",
    "def generate_embeddings(df):\n",
    "    d = co_occurrence(df)\n",
    "    trimmed = trim_d(d)\n",
    "    vocab, cooccurrence = d_to_matrix(d)\n",
    "    glove_model = GloVe(n=25, max_iter=100)\n",
    "    embeddings = glove_model.fit(cooccurrence)\n",
    "    return vocab, embeddings\n",
    "\n",
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename, encoding=\"utf8\") as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co-occurrence\n",
      "trimming\n",
      "90557\n",
      "15798\n",
      "matrixing\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(csv_name(year)) for year in [2019, 2018, 2017]], ignore_index=True, sort=False)\n",
    "d = co_occurrence(df)\n",
    "d = trim_d(d)\n",
    "vocab, cooccurrence = d_to_matrix(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded original embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: error 48720.1756"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-919bde582b35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'embeddings.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vocab.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "original_embedding = glove2dict('./glove.6B/glove.6B.50d.txt')\n",
    "print(\"loaded original embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mittens_model = Mittens(n=50, max_iter=1000)\n",
    "# Note: n must match the original embedding dimension\n",
    "new_embeddings = mittens_model.fit(\n",
    "    cooccurrence,\n",
    "    vocab=vocab,\n",
    "    initial_embedding_dict=original_embedding)\n",
    "np.savetxt('embeddings.csv', new_embeddings, delimiter=',')\n",
    "with open(\"vocab.txt\", \"wb\") as fp:\n",
    "    pickle.dump(vocab, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15798"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2010s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=28697, size=100, alpha=0.025)\n",
      "[('marijuana', 1.0), ('cannabis', 0.7156508564949036), ('drug:', 0.6136324107646942), ('recreational', 0.5932257771492004), ('hemp', 0.5360226631164551), ('tobacco', 0.5102111399173737), ('juul', 0.4959568381309509)]\n",
      "[('cannabis', 1.0), ('marijuana', 0.715650886297226), ('hemp', 0.677280455827713), ('drug:', 0.6450612246990204), ('tobacco', 0.6384859085083008), ('e-cigarette', 0.6166323721408844), ('recreational', 0.5828753709793091)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(csv_name(year)) for year in [2019, 2018, 2017]], ignore_index=True, sort=False)\n",
    "sentences = [list(map(lambda s : process_strings(s), p.split())) for p in df['text']]\n",
    "model = Word2Vec(sentences)\n",
    "print(model)\n",
    "words = list(model.wv.vocab)\n",
    "indexer = AnnoyIndexer(model, 2)\n",
    "print(model.most_similar(\"marijuana\", topn=7, indexer=indexer))\n",
    "print(model.most_similar(\"cannabis\", topn=7, indexer=indexer))\n",
    "X = model.wv[model.wv.vocab]\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2000s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co-occurrence\n",
      "trimming\n",
      "77634\n",
      "13339\n",
      "matrixing\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(csv_name(year)) for year in [2009, 2008, 2007]], ignore_index=True, sort=False)\n",
    "d = co_occurrence(df)\n",
    "d = trim_d(d)\n",
    "vocab, cooccurrence = d_to_matrix(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: error 36949.1956"
     ]
    }
   ],
   "source": [
    "mittens_model = Mittens(n=50, max_iter=1000)\n",
    "# Note: n must match the original embedding dimension\n",
    "new_embeddings = mittens_model.fit(\n",
    "    cooccurrence,\n",
    "    vocab=vocab,\n",
    "    initial_embedding_dict=original_embedding)\n",
    "np.savetxt('embeddings-00.csv', new_embeddings, delimiter=',')\n",
    "with open(\"vocab-00.txt\", \"wb\") as fp:\n",
    "    pickle.dump(vocab, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1990s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co-occurrence\n",
      "trimming\n",
      "59774\n",
      "8958\n",
      "matrixing\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(csv_name(year)) for year in [1999, 1998, 1997]], ignore_index=True, sort=False)\n",
    "d = co_occurrence(df)\n",
    "d = trim_d(d)\n",
    "vocab, cooccurrence = d_to_matrix(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: error 18650.0542"
     ]
    }
   ],
   "source": [
    "mittens_model = Mittens(n=50, max_iter=1000)\n",
    "# Note: n must match the original embedding dimension\n",
    "new_embeddings = mittens_model.fit(\n",
    "    cooccurrence,\n",
    "    vocab=vocab,\n",
    "    initial_embedding_dict=original_embedding)\n",
    "np.savetxt('embeddings-90.csv', new_embeddings, delimiter=',')\n",
    "with open(\"vocab-90.txt\", \"wb\") as fp:\n",
    "    pickle.dump(vocab, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1980s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co-occurrence\n",
      "trimming\n",
      "53220\n",
      "7809\n",
      "matrixing\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(csv_name(year)) for year in [1989, 1988, 1987]], ignore_index=True, sort=False)\n",
    "d = co_occurrence(df)\n",
    "d = trim_d(d)\n",
    "vocab, cooccurrence = d_to_matrix(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: error 15288.0125"
     ]
    }
   ],
   "source": [
    "mittens_model = Mittens(n=50, max_iter=1000)\n",
    "# Note: n must match the original embedding dimension\n",
    "new_embeddings = mittens_model.fit(\n",
    "    cooccurrence,\n",
    "    vocab=vocab,\n",
    "    initial_embedding_dict=original_embedding)\n",
    "np.savetxt('embeddings-80.csv', new_embeddings, delimiter=',')\n",
    "with open(\"vocab-80.txt\", \"wb\") as fp:\n",
    "    pickle.dump(vocab, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
